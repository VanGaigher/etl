# ETL Project with Google BigQuery, Python, PostgreSQL, and Power BI

This project aims to demonstrate the process of creating an ETL (Extract, Transform, Load) pipeline to extract data from a data warehouse hosted on Google BigQuery, transform it using Python, load it into a PostgreSQL database, and finally build metrics and a Dashboard using Power BI as the final product.

## Step-by-step process:

### 1. Extraction (Extract)
- The data will be extracted from a data warehouse hosted on Google BigQuery. This data warehouse contains the datasets necessary for analysis.

### 2. Transformation (Transform)
- The extracted data will then undergo a transformation process. This process will be carried out using the Python language, where various operations will be applied for cleaning, normalization, and aggregation of data as necessary. This step is crucial to ensure the quality and consistency of the data before it is loaded into the final database.

### 3. Loading (Load)
- After the data transformation, the next step is to load it into a PostgreSQL database. This will be done using appropriate tools and libraries in Python to establish a connection with the database and load the transformed data into corresponding tables.

  ## Project Schema:

![project_schema](https://github.com/VanGaigher/ETL_SalesRetail/blob/main/Project_schema.png)


### 4. Building Metrics and Dashboard
- Once the data is loaded into PostgreSQL, it will be used to build metrics and a Dashboard in Power BI. Power BI offers a variety of tools and resources for creating interactive and informative data visualizations. The final Dashboard will be a visual representation of the insights obtained from the transformed data, allowing for deeper analysis and better understanding of patterns and trends.

![dashboard_salesreport](https://github.com/VanGaigher/ETL_SalesRetail/blob/main/dashboard.gif)


